{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ussin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ussin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "#punkt and stopqwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../CSE508_Winter2023_Dataset/CSE508_Winter2023_Dataset_XML/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    # open the file and read the text\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Perform tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove punctuations but not hyphen separated words\n",
    "    tokens = [w for w in tokens if w.isalpha() or '-' in w]\n",
    "    # Remove blank space tokens\n",
    "    tokens = [w for w in tokens if w.strip()]\n",
    "    return tokens\n",
    "\n",
    "# make a dictionary of bigram tokens as keys containg the list of files they occur in\n",
    "bigram_dict = {}\n",
    "def make_dict(filename, tokens):\n",
    "    for i in range(len(tokens)-1):\n",
    "        bigram = tokens[i] + ' ' + tokens[i+1]\n",
    "        if bigram in bigram_dict:\n",
    "            bigram_dict[bigram].append(filename)\n",
    "        else:\n",
    "            bigram_dict[bigram] = [filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0766d3be5b4a49ed8392fb6a5662dcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the function preprocess and make_dict for all the files in the directory\n",
    "for filename in tqdm(os.listdir()):\n",
    "    tokens = preprocess(filename)\n",
    "    make_dict(filename, tokens)\n",
    "\n",
    "# save the dictionary in a pickle file\n",
    "os.chdir('../InvertedIndex')\n",
    "# delete file with given filename in directory\n",
    "if os.path.exists('bigramindex.pickle'):\n",
    "    os.remove('bigramindex.pickle')\n",
    "\n",
    "# save the dictionary in a pickle file\n",
    "with open('bigramindex.pickle', 'wb') as f:\n",
    "    pickle.dump(bigram_dict, f)\n",
    "\n",
    "# delete file with given filename in directory\n",
    "if os.path.exists('bigramindex.txt'):\n",
    "    os.remove('bigramindex.txt')\n",
    "\n",
    "with open('bigramindex.txt', 'w') as f:\n",
    "    f.write(json.dumps(bigram_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigramindex.pickle', 'rb') as handle:\n",
    "    bigramindex = pickle.load(handle)\n",
    "\n",
    "def get_posting_list(term):\n",
    "    if term in bigramindex:\n",
    "        return bigramindex[term]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def and_query(posting_list1, size1, posting_list2, size2):\n",
    "    result = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < size1 and j < size2:\n",
    "        if posting_list1[i] == posting_list2[j]:\n",
    "            result.append(posting_list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif posting_list1[i] < posting_list2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result\n",
    "\n",
    "def bigram_query(query):\n",
    "    # query = input(\"Enter your query: \")\n",
    "    # query = preprocess(query)\n",
    "    query = query.lower()\n",
    "    # Perform tokenization\n",
    "    tokens = word_tokenize(query)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove punctuations but not hyphen separated words\n",
    "    tokens = [w for w in tokens if w.isalpha() or '-' in w]\n",
    "    # Remove blank space tokens\n",
    "    tokens = [w for w in tokens if w.strip()]\n",
    "    # query = set(tokens)\n",
    "    query = list(tokens)\n",
    "    # print(query)\n",
    "    bi_query = []\n",
    "\n",
    "    for i in range(len(query)-1):\n",
    "        bi_query.append(query[i] + ' ' + query[i+1])\n",
    "    length = len(bi_query)-1\n",
    "    result = get_posting_list(bi_query[0])\n",
    "    size = len(result)\n",
    "    itr = 0\n",
    "    while itr < length:\n",
    "        result = and_query(result, size, get_posting_list(bi_query[itr+1]), len(get_posting_list(bi_query[itr+1])))\n",
    "        size = len(result)\n",
    "        itr += 1\n",
    "    return result\n",
    "    # if query in bigram_dict:\n",
    "    #     print(\"The query is present in the following files:\")\n",
    "    #     for file in bigram_dict[query]:\n",
    "    #         print(file)\n",
    "    # else:\n",
    "    #     print(\"The query is not present in any file.\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cranfield1396']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_query('possess in order to support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../CSE508_Winter2023_Dataset_XML/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}\n",
    "\n",
    "# dictionary of token as key and list of wehere the filename itself is dictionary and positions as value\n",
    "def make_dict(tokens, filename):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in db:\n",
    "            if filename in db[tokens[i]]:\n",
    "                db[tokens[i]][filename].append(i)\n",
    "            else:\n",
    "                db[tokens[i]][filename] = [i]\n",
    "        else:\n",
    "            db[tokens[i]] = {filename: [i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f2f104b28f4c7e9ad357de5924f0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir()):\n",
    "    tokens = preprocess(filename)\n",
    "    make_dict(tokens, filename)\n",
    "\n",
    "# save the dictionary in a pickle file\n",
    "os.chdir('../InvertedIndex')\n",
    "# delete file with given filename in directory\n",
    "if os.path.exists('positional.pickle'):\n",
    "    os.remove('positional.pickle')\n",
    "\n",
    "# save the dictionary in a pickle file\n",
    "with open('positional.pickle', 'wb') as f:\n",
    "    pickle.dump(db, f)\n",
    "\n",
    "# delete file with given filename in directory\n",
    "if os.path.exists('positional.txt'):\n",
    "    os.remove('positional.txt')\n",
    "\n",
    "with open('positional.txt', 'w') as f:\n",
    "    f.write(json.dumps(db, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../InvertedIndex')\n",
    "\n",
    "# load positional index pickle file\n",
    "with open('positional.pickle', 'rb') as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "\n",
    "# positional query function that takes a query as input and returns the files that contain the query in the order of the query\n",
    "def positional_query(query):\n",
    "    query = query.lower()\n",
    "    tokens = word_tokenize(query)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove punctuations but not hyphen separated words\n",
    "    tokens = [w for w in tokens if w.isalpha() or '-' in w]\n",
    "    # remove blank space tokens\n",
    "    tokens = [w for w in tokens if w.strip()]\n",
    "    # the db stored in the format of {token: {filename: [positions]}}\n",
    "    # find the files that contain the first word in the query\n",
    "    files = db[tokens[0]]\n",
    "    # find the files that contain the rest of the words in the query\n",
    "    for i in range(1, len(tokens)):\n",
    "        files = [file for file in files if file in db[tokens[i]]]\n",
    "    # find the files that contain the words in the query in the order of the query also they should be also if they are at same distance using position as in the query the disctionary stores {token: {filename: [positions]}}\n",
    "    for file in files:\n",
    "        positions = [db[tokens[0]][file]]\n",
    "        for i in range(1, len(tokens)):\n",
    "            positions.append(db[tokens[i]][file])\n",
    "        # check if the positions are in the order of the query and also if they are at same distance using position as in the query the disctionary stores {token: {filename: [positions]}}\n",
    "        for i in range(len(positions)-1):\n",
    "            if(positions[i][-1] > positions[i+1][0]):\n",
    "                files.remove(file)\n",
    "                break\n",
    "        \n",
    "    return files\n",
    "\n",
    "# get posting list function for positional index\n",
    "def get_posting_list(token):\n",
    "    if token in db:\n",
    "        return list(db[token].items())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "#positional intersect function which takes 2 posting list and takes distance as input and check is docid is same and if the positions are at same distance\n",
    "# def positional_intersect(posting_list1, posting_list2, distance=1):\n",
    "#     result = []\n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     while i < len(posting_list1) and j < len(posting_list2):\n",
    "#         if posting_list1[i][0] == posting_list2[j][0]:\n",
    "#             if 1 <= posting_list1[i][1][-1] - posting_list2[j][1][0]  <= distance:\n",
    "#                 result.append(posting_list1[i])\n",
    "#             i += 1\n",
    "#             j += 1\n",
    "#         elif posting_list1[i][0] < posting_list2[j][0]:\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             j += 1\n",
    "#     return result\n",
    "\n",
    "# function that takes query and preprocesses it to get tokens iterate over it to get the posting list of each token and then intersect them to get the final result\n",
    "# def positional_query(query):\n",
    "#     query = query.lower()\n",
    "#     tokens = word_tokenize(query)\n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "#     # remove punctuations\n",
    "#     tokens = [w for w in tokens if w.isalpha() or '-' in w]\n",
    "#     # remove blank space tokens\n",
    "#     tokens = [w for w in tokens if w.strip()]\n",
    "#     # the db stored in the format of {token: {filename: [positions]}}\n",
    "#     # find the files that contain the first word in the query\n",
    "#     posting_list = get_posting_list(tokens[0])\n",
    "#     # find the files that contain the rest of the words in the query distance is the differece between psitions of the words in the query is 1 and step size is 1\n",
    "#     for i in range(1, len(tokens)):\n",
    "#          #distance between the words in the query is the third argumnet\n",
    "#         posting_list = positional_intersect(posting_list, get_posting_list(tokens[i]), 1)\n",
    "#     # now check same posting list for words seperated  by 2 words\n",
    "#     for i in range(1, len(tokens),2):\n",
    "#         posting_list = positional_intersect(posting_list, get_posting_list(tokens[i]), 2)\n",
    "#     # now check same posting list for words seperated  by 3 words\n",
    "#     for i in range(1, len(tokens),3):\n",
    "#         posting_list = positional_intersect(posting_list, get_posting_list(tokens[i]), 3)\n",
    "#     # now check same posting list for words seperated  by 4 words\n",
    "#     for i in range(1, len(tokens),4):\n",
    "#         posting_list = positional_intersect(posting_list, get_posting_list(tokens[i]), 4)\n",
    "#     # now check same posting list for words seperated  by 5 words\n",
    "#     for i in range(1, len(tokens),5):\n",
    "#         posting_list = positional_intersect(posting_list, get_posting_list(tokens[i]), 5)\n",
    "\n",
    "#     return posting_list\n",
    "# for i in range(len(positions)-1):\n",
    "#             if positions[i][-1] > positions[i+1][0]:\n",
    "#                 files.remove(file)\n",
    "#                 break\n",
    "\n",
    "\n",
    "# for i in range(len(positions)-1):\n",
    "        #     d = 0\n",
    "        #     if not(d >= positions[i+1][0] - positions[i][-1] >= 1):\n",
    "        #         files.remove(file)\n",
    "        #         break\n",
    "\n",
    "\n",
    "def positional_query(query, distance=1):\n",
    "    query = query.lower()\n",
    "    tokens = word_tokenize(query)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove punctuations\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # remove blank space tokens\n",
    "    tokens = [w for w in tokens if w.strip()]\n",
    "    # find the files that contain the first word in the query\n",
    "    files = db[tokens[0]]\n",
    "    # find the files that contain the rest of the words in the query\n",
    "    for i in range(1, len(tokens)):\n",
    "        files = [file for file in files if file in db[tokens[i]]]\n",
    "    # find the files that contain the words in the query in the order of the query\n",
    "\n",
    "\n",
    "    result = []\n",
    "    for file in files:\n",
    "        positions = [db[tokens[0]][file]]\n",
    "        for i in range(1, len(tokens)):\n",
    "            # final=[]\n",
    "            positions.append(db[tokens[i]][file])\n",
    "        for i in range(len(positions)-1):\n",
    "            final=[]\n",
    "            a =0\n",
    "            b =0\n",
    "            pos1=positions[i]\n",
    "            pos2=positions[i+1]\n",
    "            while a < len(pos1) and b < len(pos2):\n",
    "                if (distance >= pos2[b]-pos1[a] >= 1):\n",
    "                    final.append(pos2[b])\n",
    "                    a+=1\n",
    "                    b+=1\n",
    "                elif pos1[a] > pos2[b]:\n",
    "                    b += 1\n",
    "                else:\n",
    "                    a += 1\n",
    "            positions[i+1]=final\n",
    "        if len(positions[-1]) != 0:\n",
    "            result.append(file)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using bigram inverted index: 0\n",
      "Names of documents retrieved for query 1 using using bigram inverted index: []\n",
      "Number of documents retrieved for query 1 using positional index: 1\n",
      "Names of documents retrieved for query 1 using positional index: ['cranfield1396']\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    n = int(input())\n",
    "    queries=[]\n",
    "    for j in range(n):\n",
    "        s = input()\n",
    "        queries.append(s)\n",
    "    \n",
    "    for i in range(n):\n",
    "        query1 = queries[i]\n",
    "        query2 = queries[i]\n",
    "        bi_result = bigram_query(query1)\n",
    "        po_result = positional_query(query2)\n",
    "        print(\"Number of documents retrieved for query {} using bigram inverted index: {}\".format(i+1, len(bi_result)))\n",
    "        print(\"Names of documents retrieved for query {} using using bigram inverted index: {}\".format(i+1, bi_result))\n",
    "        print(\"Number of documents retrieved for query {} using positional index: {}\".format(i+1, len(po_result)))\n",
    "        print(\"Names of documents retrieved for query {} using positional index: {}\".format(i+1, po_result))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
