{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../CSE508_Winter2023_Dataset/InvertedIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the invertedindex data\n",
    "with open('invertedindex.pickle', 'rb') as handle:\n",
    "    invertedindex = pickle.load(handle)\n",
    "\n",
    "def get_posting_list(term):\n",
    "    if term in invertedindex:\n",
    "        return invertedindex[term][0]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_posting_listsize(term):\n",
    "    if term in invertedindex:\n",
    "        return invertedindex[term][1]\n",
    "\n",
    "os.chdir('../CSE508_Winter2023_Dataset')\n",
    "all_docs = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and query and number of minimum iterations to find and of two terms\n",
    "def and_query(term1, term2):\n",
    "    posting_list1 = get_posting_list(term1)\n",
    "    posting_list2 = get_posting_list(term2)\n",
    "    posting_list1_size = get_posting_listsize(term1)\n",
    "    posting_list2_size = get_posting_listsize(term2)\n",
    "    return and_query_helper(posting_list1, posting_list1_size, posting_list2, posting_list2_size)\n",
    "\n",
    "def and_query_helper(posting_list1, size1, posting_list2, size2):\n",
    "    result = []\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < size1 and j < size2:\n",
    "        count += 1\n",
    "        if posting_list1[i] == posting_list2[j]:\n",
    "            result.append(posting_list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif posting_list1[i] < posting_list2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result, count\n",
    "\n",
    "\n",
    "def and_not_query(term1, term2):\n",
    "    posting_list1 = get_posting_list(term1)\n",
    "    posting_list2 = get_posting_list(term2)\n",
    "    posting_list1_size = get_posting_listsize(term1)\n",
    "    posting_list2_size = get_posting_listsize(term2)\n",
    "    return and_not_query_helper(posting_list1, posting_list1_size, posting_list2, posting_list2_size)\n",
    "\n",
    "\n",
    "def and_not_query_helper(posting_list1, size1, posting_list2, size2):\n",
    "    result = []\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < size1 and j < size2:\n",
    "        count += 1\n",
    "        if posting_list1[i] == posting_list2[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif posting_list1[i] < posting_list2[j]:\n",
    "            result.append(posting_list1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    while i < size1:\n",
    "        count+=1\n",
    "        result.append(posting_list1[i])\n",
    "        i += 1\n",
    "    return result, count\n",
    "\n",
    "def or_query(term1, term2):\n",
    "    posting_list1 = get_posting_list(term1)\n",
    "    posting_list2 = get_posting_list(term2)\n",
    "    posting_list1_size = get_posting_listsize(term1)\n",
    "    posting_list2_size = get_posting_listsize(term2)\n",
    "    return or_query_helper(posting_list1, posting_list1_size, posting_list2, posting_list2_size)\n",
    "\n",
    "\n",
    "def or_query_helper(posting_list1, size1, posting_list2, size2):\n",
    "    result = []\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    while i < size1 and j < size2:\n",
    "        count += 1\n",
    "        if posting_list1[i] == posting_list2[j]:\n",
    "            result.append(posting_list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif posting_list1[i] < posting_list2[j]:\n",
    "            result.append(posting_list1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(posting_list2[j])\n",
    "            j += 1\n",
    "    if i < size1:\n",
    "        count += 1\n",
    "        result.extend(posting_list1[i::])\n",
    "    if j < size2:\n",
    "        count += 1\n",
    "        result.extend(posting_list2[j::])\n",
    "    return result, count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def or_not_query_helper(posting_list1, size1, posting_list2, size2):\n",
    "    result = []\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    # all_docs is the list of all documents in the dataset taking difference of alldocs and postinglist2\n",
    "    # not of postinglist2\n",
    "    diff = list(set(all_docs) - set(posting_list2))\n",
    "    #or of posting list1 and not of postinglist2\n",
    "    result, count = or_query_helper(posting_list1, size1, diff, len(diff))\n",
    "    return result, count\n",
    "\n",
    "\n",
    "def or_not_query(term1, term2):\n",
    "    posting_list1 = get_posting_list(term1)\n",
    "    posting_list2 = get_posting_list(term2)\n",
    "    posting_list1_size = get_posting_listsize(term1)\n",
    "    posting_list2_size = get_posting_listsize(term2)\n",
    "    return or_query_helper(posting_list1, posting_list1_size, posting_list2, posting_list2_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now input and output\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query  2 :  fluid AND velocity\n",
      "Number of documents retrieved for query  2 :  65\n",
      "Names of the documents retrieved for query  2 :  ['cranfield0004', 'cranfield0014', 'cranfield0044', 'cranfield0059', 'cranfield0061', 'cranfield0071', 'cranfield0072', 'cranfield0073', 'cranfield0087', 'cranfield0088', 'cranfield0106', 'cranfield0112', 'cranfield0128', 'cranfield0131', 'cranfield0133', 'cranfield0139', 'cranfield0151', 'cranfield0191', 'cranfield0241', 'cranfield0257', 'cranfield0260', 'cranfield0267', 'cranfield0268', 'cranfield0269', 'cranfield0296', 'cranfield0299', 'cranfield0321', 'cranfield0351', 'cranfield0352', 'cranfield0397', 'cranfield0414', 'cranfield0417', 'cranfield0452', 'cranfield0460', 'cranfield0474', 'cranfield0475', 'cranfield0477', 'cranfield0550', 'cranfield0559', 'cranfield0653', 'cranfield0770', 'cranfield0771', 'cranfield0773', 'cranfield0787', 'cranfield0788', 'cranfield0872', 'cranfield0962', 'cranfield0963', 'cranfield0967', 'cranfield0987', 'cranfield1072', 'cranfield1082', 'cranfield1149', 'cranfield1157', 'cranfield1222', 'cranfield1238', 'cranfield1251', 'cranfield1273', 'cranfield1283', 'cranfield1302', 'cranfield1327', 'cranfield1335', 'cranfield1368', 'cranfield1370', 'cranfield1375']\n",
      "Number of comparisons required for query  2 :  359\n"
     ]
    }
   ],
   "source": [
    "# take an input n denoting number of queries\n",
    "# the next 2*n lines contain queries in the format, input sequence in first line followed by the operations to be performed in next line.\n",
    "\n",
    "def main():\n",
    "    n = int(input())\n",
    "    phrase = []\n",
    "    ops = []\n",
    "    for _ in range(n):\n",
    "        input_sequence = input()\n",
    "        operations = input()\n",
    "        phrase.append(input_sequence)\n",
    "        ops.append(operations)\n",
    "\n",
    "    for i in range(n):\n",
    "        query = phrase[i]\n",
    "        # query = query.split()\n",
    "        query = query.lower()\n",
    "        # Perform tokenization\n",
    "        tokens = word_tokenize(query)\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # remove punctuations but not hyphen separated words\n",
    "        tokens = [w for w in tokens if w.isalpha() or '-' in w]\n",
    "        # Remove blank space tokens\n",
    "        tokens = [w for w in tokens if w.strip()]\n",
    "        query = set(tokens)\n",
    "        op = ops[i]\n",
    "        op = op.split(',')\n",
    "\n",
    "        query = list(query)\n",
    "        result = get_posting_list(query[0])\n",
    "        size = get_posting_listsize(query[0])\n",
    "        itr = 0\n",
    "        TotalCount =0\n",
    "        # print(op)\n",
    "        # print(query)\n",
    "        while(itr<len(op)):\n",
    "            if op[itr] == 'AND':\n",
    "                result, count = and_query_helper(result, size, get_posting_list(query[itr+1]), get_posting_listsize(query[itr+1]))\n",
    "                \n",
    "                TotalCount += count\n",
    "                size = len(result)\n",
    "            elif op[itr] == 'OR':\n",
    "                # print(query[itr+1])\n",
    "                result, count = or_query_helper(result, size, get_posting_list(query[itr+1]), get_posting_listsize(query[itr+1]))\n",
    "                # result = set(result)\n",
    "                TotalCount += count\n",
    "                size = len(result)\n",
    "            elif op[itr] == 'AND NOT':\n",
    "                # print(query[itr+1])\n",
    "                result, count = and_not_query_helper(result, size, get_posting_list(query[itr+1]), get_posting_listsize(query[itr+1]))\n",
    "                # result = set(result)\n",
    "                TotalCount += count\n",
    "                size = len(result)\n",
    "            elif op[itr] == 'OR NOT':\n",
    "                result, count = or_not_query_helper(result, size, get_posting_list(query[itr+1]), get_posting_listsize(query[itr+1]))\n",
    "                # result = set(result)\n",
    "                TotalCount += count\n",
    "                size = len(result)\n",
    "            itr += 1\n",
    "        string = query[0]\n",
    "        for i in range(1,len(query)):\n",
    "            string += ' '+op[i-1]+' '+query[i]\n",
    "        print(\"Query \",n+1,\": \",string)\n",
    "        print(\"Number of documents retrieved for query \",n+1,\": \",len(result))\n",
    "        print(\"Names of the documents retrieved for query \",n+1,\": \",result)\n",
    "        print(\"Number of comparisons required for query \",n+1,\": \",TotalCount)\n",
    "\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
